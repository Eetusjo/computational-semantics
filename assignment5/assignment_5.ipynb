{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Semantics 2018 (LDA-T3101)\n",
    "## Practical assignment 5: Paraphrase identification with neural networks\n",
    "---\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "This assignment is shared as a [Jupyter Notebook](https://jupyter.org) document. A notebook is an interactive document that contains a mix of executable code and Markdown elements, among others. A notebook is divided into cells, and you can identify a cell by the bounded box that surrounds it when you select the cell. The cell you are reading right now is a Markdown cell. We will use Markdown cells to structure the assignment and give you the task descriptions.\n",
    "\n",
    "The other type of cell we are interested in is the Code cell. The cell below that contains a `print`-command is a Code cell. You can run the cell by selecting it and pressing `Ctrl+Enter`. Run it and you should see the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code cell\n",
    "a = 1 + 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code execution is handled by a Python kernel that runs in the background. When you run a code cell, any variables you create will be stored by the kernel. The whole notebook shares a single kernel, so you can reuse the variables in later cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This offers a convenient way of structuring the code. It is especially convenient when some of the code is very slow to run or only needs to be run once. This is the case for this assignment: for example, you will only need to prepare the data you will use once, but you probably need to change a function that defines your neural network model several times. Because preparing the data is rather slow, only running it once speeds up development.\n",
    "\n",
    "Notice that this feature of the notebook can also lead to bugs: because variables are stored in the kernel, you can define a variable, run the code, delete the definition, and use the variable somewhere without the original definition being present. If you face hard-to-find bugs in your code, clearing the kernel of all variables is an option. You can restart the kernel by choosing `Kernel -> Restart` from the top menu.\n",
    "\n",
    "This should be enough to get you started. Jupyter Notebook contains a ton of advanced features, but you do not need them to complete the assignment. If you are interested in diving deeper, check out the [Jupyter website](https://jupyter.org) and the help menu.\n",
    "\n",
    "---\n",
    "**IMPORTANT:** The CSC Notebooks environments are destroyed after a set time period. This means that unless you complete the assignment in one sitting and within the alloted time, you need to download the notebook and upload it later. No changes to this notebook will be saved otherwise. You can dowload the notebook using the top menu: `File -> Download as -> Notebook (.ipynb)`. You can later continue the assignment by uploading the notebook to a new environment (`upload`-button in the upper right corner of the directory view when you open a new environment). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Developing neural networks\n",
    "---\n",
    "\n",
    "One way of structuring the process of developing a neural network model for any task is the following:\n",
    "\n",
    "1. Data preparation\n",
    "2. Deciding the model architecture\n",
    "3. Training the model\n",
    "3. Evaluating the model\n",
    "\n",
    "This assignment is structured similarly. We will start by importing a bunch of stuff we need. Press `Ctrl+Enter` in the code cell below and move on to Part 1 of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy using workaround\n",
    "# !pip install spacy --quiet\n",
    "# !python -m spacy download en\n",
    "# Install pydot for plotting model\n",
    "# !pip install pydot --quiet\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "import utils\n",
    "\n",
    "from IPython.display import Image\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, LSTM, Embedding, Concatenate, Dense, Dropout, Masking\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# DO NOT CHANGE THESE\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(234)\n",
    "\n",
    "# Load spaCy utilities for English\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 1: Data preparation (2 points)\n",
    "---\n",
    "\n",
    "The task you will perform in this assignment is *paraphrase identification*. As the name suggests, in the paraphrase identification task the model is given two sentences and it should decide whether the sentences are paraphrases or not. We will use the [**Quora question pairs**](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) corpus.\n",
    "\n",
    "In this first part of the assignment we will load and preprocess the data so that we can actually feed it into our network. \n",
    "\n",
    "The code cell below reads the data file. Each line of the file contains a single sample as 6 different tab-separated fields. The `lines` variable will be a dictionary `lineID -> data`, where `lineID` is a unique identifier for each data sample and `data` is another dictionary with keys `srcID`, `tgtID`, `source`, `target`, and `label`.\n",
    "\n",
    "* **srcID**: a unique identifier of the source sentence\n",
    "* **tgtID**: a unique identifier of the target sentence\n",
    "* **source**: the source sentence\n",
    "* **target**: the target sentence\n",
    "* **label**: a label indicating whether the sentences are paraphrases (1) or not (0)\n",
    "\n",
    "Run the code cell to read the data file. You do not need to change the parameters when calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change\n",
    "lines = utils.load_lines(\n",
    "    file_name=\"data/quora/quora_duplicate_questions.tsv\",\n",
    "    fields=\"lineID srcID tgtID source target label\".split(),\n",
    "    delimiter=\"\\t\",\n",
    "    n=200000,\n",
    "    skip_first=True\n",
    ")\n",
    "print(\"First sample in the data:\")\n",
    "print(lines[\"ID0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to prepare the data for actual usage. The cell below gathers tokenizes the data and gathers the source and target sentences into two separate lists. The correct labels will be gathered in a third list. You do not need to change this code but check it out to understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a spaCy tokenizer (https://spacy.io)\n",
    "tokenizer = English().Defaults.create_tokenizer(nlp)\n",
    "\n",
    "# Initialize empty lists for source and target sentences as well as labels.\n",
    "src_strings = []\n",
    "tgt_strings = []\n",
    "labels = []\n",
    "\n",
    "# Go through the read lines\n",
    "for line_id, line_obj in lines.items():\n",
    "    # Tokenize the sentences\n",
    "    src = \" \".join([token.text for token in tokenizer(line_obj[\"source\"])])\n",
    "    tgt = \" \".join([token.text for token in tokenizer(line_obj[\"target\"])])\n",
    "    # Extract label\n",
    "    label = int(line_obj[\"label\"])\n",
    "    \n",
    "    src_strings.append(src)\n",
    "    tgt_strings.append(tgt)\n",
    "    labels.append(label)\n",
    "    \n",
    "# Print examples of tokenized data\n",
    "for i in range(10):\n",
    "    print(\"{}\\t{}\\t{}\".format(src_strings[i], tgt_strings[i], labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step you have the tokenized data split into three lists. You now need to implement the following streps:\n",
    "\n",
    "1. **Learn the vocabulary of the text and the mapping from tokens to integers:**\n",
    "    \n",
    "        \"can\" -> 2\n",
    "        \"you\" -> 3\n",
    "         etc.\n",
    "      \n",
    "2. **Map each word sequence into an sequence of integers:**\n",
    "\n",
    "        \"Can you fill the can ?\" -> [2, 3, 14, 25, 2, 53]\n",
    "        \n",
    "        Note: This is simply a preprocessing step needed for feeding the sentence to the network. The resulting sequence is not a distributional representation of the sentence.\n",
    "        \n",
    "3. **Pad each sequence to length 'MAX_LEN' using the padding index 0:**\n",
    "\n",
    "        [2, 3, 15, 25, 2, 53] -> [0, 0, 0, ..., 2, 3, 15, 25, 2, 53]\n",
    "        \n",
    "In addition to implementing each of the three steps above, after each step print out at least a part of the data  (for example a single sample) and explain as comments in the cell what the data looks like and why. The cell below contains some code that will get you started, as well as hints for each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORDS = 50000  # Vocabulary size\n",
    "MAX_LEN = 50  # Maximum length of sentence (in tokens)\n",
    "\n",
    "# STEP 1. Learning the vocabulary\n",
    "# Below we initialize a Keras Tokenizer (keras.io/preprocessing/text/) that you can\n",
    "# use to learn the mapping from tokens to integers. It will automatically limit the\n",
    "# vocabulary to the 'N_WORDS' most common tokens, lowercase the data, and replace \n",
    "# all out-of-vocabulary tokens (those outside the 'N_WORDS' most common) with a\n",
    "# special symbol (<UNK>).\n",
    "processor = Tokenizer(num_words=N_WORDS, lower=True, oov_token=\"<UNK>\")\n",
    "\n",
    "# TODO: the method 'fit_on_texts' does the actual learning. It takes in a list of strings\n",
    "# (sentences) and constructs the vocabulary and mapping. Change the line below so that \n",
    "# it learns the vocabulary from the Quora data.\n",
    "processor.fit_on_texts([\"hello how are you ?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2. Mapping the sentences\n",
    "# TODO: Now that we have learned the mapping, we need to use it to map the sentences in our data\n",
    "# to integer sequences. 'processor' has a method called 'texts_to_sequences' that takes in\n",
    "# a list of strings and return the mapped sentences. Create new lists 'src_mapped' and 'tgt_mapped'\n",
    "# that contain the mapped source and target sentences respectively.\n",
    "print(processor.texts_to_sequences([\"hello how are you ?\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3. Padding\n",
    "# At this point 'src_mapped' and 'tgt_mapped' are lists of variable-length arrays.\n",
    "# In order to feed a batch of sentences into our model, we need to do so-called\n",
    "# padding. In padding we extend the shorter sequences to the length of the longest\n",
    "# sequence by using a special padding symbol, in this case the integer 0. You can\n",
    "# do this with the method 'processor.pad_sequences', which takes in the mapped \n",
    "# sentences and a keyword argument 'maxlen' which is the padding length\n",
    "#\n",
    "# The 'pad_sequences'-function also constructs a matrix out of the training sequences.\n",
    "# At this step the lists 'src_mapped' and 'tgt_mapped' are turned into matrices with \n",
    "# dimensions (n_training_pairs x MAX_LEN).\n",
    "#\n",
    "# TODO: Create two variables 'src' and 'tgt' that contain the matrices with padded sequences.\n",
    "# Make sure the matrices have the correct dimensions (attribute src.shape).\n",
    "print(pad_sequences([[1, 2, 3, 4], [5, 6, 7]], maxlen=MAX_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 2: Defining the model (1 point)\n",
    "---\n",
    "The function in the cell below defines our neural network model. Go through the code so that you undestand how it works. The cell will also draw the model for you. Explain the structure of the model as comments in the cell. The comments contain some explanation, but you should also refer to the [Keras documentation](https://keras.io) (especially the sections `Models` and `Layers`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def define_model(max_len, num_words, learning_rate, embedding_dim, lstm_units):\n",
    "    # Define model inputs (in this case, two sequences of length 'MAX_LEN')\n",
    "    src = Input(shape=(max_len,), dtype='int32')\n",
    "    tgt = Input(shape=(max_len,), dtype='int32')\n",
    "    \n",
    "    # Mask the inputs so that we do not waste computation on padding\n",
    "    src_masked = Masking(mask_value=0)(src)\n",
    "    tgt_masked = Masking(mask_value=0)(tgt)\n",
    "\n",
    "    # Define the embedding layer\n",
    "    embed = Embedding(\n",
    "        num_words, embedding_dim, \n",
    "        input_length=max_len, \n",
    "        mask_zero=True\n",
    "    )\n",
    "    # We use the 'functional' way of defining models in Keras so the embedding \n",
    "    # is done by calling the embedding layer on the sequences. Let's do it for \n",
    "    # both the source and the target.\n",
    "    embedded_src = embed(src_masked)\n",
    "    embedded_tgt = embed(tgt_masked)\n",
    "\n",
    "    # Define the encoder, in this case a simple LSTM layer.\n",
    "    encode = LSTM(units=lstm_units)\n",
    "\n",
    "    # Call the LSTM layer, this time on the embedded inputs.\n",
    "    # The outputs are the encoded sequences.\n",
    "    encoded_src = encode(embedded_src)\n",
    "    encoded_tgt = encode(embedded_tgt)\n",
    "\n",
    "    # Concatenate the encoded sequences. This serves as an input\n",
    "    # to the classification layer.\n",
    "    concatenated = Concatenate()([encoded_src, encoded_tgt])\n",
    "\n",
    "    # Classification layer 1\n",
    "    out = Dense(128, activation=\"tanh\")(concatenated)\n",
    "    # Classification layer 2\n",
    "    out = Dense(128, activation=\"tanh\")(out)\n",
    "    # Prediction layer\n",
    "    predictions = Dense(1, activation=\"sigmoid\")(out)\n",
    "    \n",
    "    # Initialize a model instance with the input layers 'src' and 'tgt'\n",
    "    # And the output 'predictions'.\n",
    "    model = Model(inputs=[src, tgt], outputs=predictions)\n",
    "    \n",
    "    # Initialize optimizer with the chosen learning rate.\n",
    "    optim = optimizers.Adam(lr=learning_rate)\n",
    "    # Compile the model.\n",
    "    model.compile(optimizer=optim, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Here we set some hyperparameters.\n",
    "EMB_BASELINE = 128  # Embedding dimensions\n",
    "UNITS_BASELINE = 128  # Number of units in the LSTM layer\n",
    "LEARNING_RATE_BASELINE = 0.005  # Learning rate for optimizer\n",
    "\n",
    "N_SAMPLES = 75000  # Number of samples to use (must be less than what we loaded above)\n",
    "\n",
    "# Create the model\n",
    "model_baseline = define_model(MAX_LEN, N_WORDS, LEARNING_RATE_BASELINE, EMB_BASELINE, UNITS_BASELINE)\n",
    "\n",
    "# Draw the model scheme\n",
    "plot_model(model_baseline, to_file='model_baseline.png', show_shapes=True, show_layer_names=True)\n",
    "Image(\"model_baseline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 3: Training the model (4 points)\n",
    "---\n",
    "\n",
    "The cell below creates a model and trains it with the given hyperparameters. This serves as a baseline for the assignment. Your task is to interpret the output of the training, evaluate the results and identify any problems with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is where the actual training happens\n",
    "history_baseline = model_baseline.fit(\n",
    "    x=[src[:N_SAMPLES], tgt[:N_SAMPLES]],  # Use N_SAMPLES first samples as data\n",
    "    y=labels[:N_SAMPLES],  # Feed in correct labels\n",
    "    batch_size=128,  # Process 128 pairs per batch\n",
    "    epochs=4,  # Go through the data 4 times \n",
    "    validation_split=0.02  # use 2% of the data for validation\n",
    ")\n",
    "\n",
    "# TODO: The training (model_baseline.fit) logs some basic information as the training\n",
    "# advances. Interpret the output with a few sentences as comments here.\n",
    "\n",
    "# TODO: The function 'utils.plot_training(history_baseline) plots some of the information \n",
    "# for another view. Identify what is problematic with the training results. Based on the\n",
    "# reading for this assignment, how would you alleviate the problem? Answer as comments.\n",
    "# Also, change the model or the training procedure and see if you can get better results.\n",
    "# (HINT: https://keras.io/layers/recurrent/#lstm, https://keras.io/regularizers/)\n",
    "#\n",
    "# You can copy the 'define_model' function to another cell or modify the existing code.\n",
    "# Also, feel free to change the amount of data you use, or the hyperparameters for training\n",
    "# if you think this is necessary. \n",
    "#\n",
    "# Because training a model can take a very long time, we do not assume you will get spectacular\n",
    "# results. You can get full points if you identify the problem, implement some way of\n",
    "# alleviating it, and train a model with some sensible parameters, even if the results do not\n",
    "# improve significantly.\n",
    "\n",
    "utils.plot_training(history_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 4: Evaluating the model (3 points)\n",
    "---\n",
    "\n",
    "In this last part you need to test the model on some of your own data. Usually at this point the model would be evaluated on a test set that has not been seen during the development. This time we will skip that, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Figure out a way to test the model on your own sentences. (HINT: \n",
    "# https://keras.io/models/model/). Discuss how well the model performs.\n",
    "\n",
    "test_pairs = [\n",
    "    (\"Why are carrots orange ?\", \"What is the reason for carrots' orange color ?\")\n",
    "    # etc ..\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
